{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9dad51",
   "metadata": {},
   "source": [
    "# ERA5 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1539dab",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5671d",
   "metadata": {},
   "source": [
    "### Download Dataset with API\n",
    "**Requirement:** Setup the cdsapi with your account as described here: https://cds.climate.copernicus.eu/how-to-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b150e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"era5-land-monthly-means-1992-2020\"\n",
    "target_zip_file = f'data/era5_climate_data/{dataset_name}.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa76a8f",
   "metadata": {},
   "source": [
    "1. Make the API call. If the internet connection is unstable the download tends to fail through the API, in that case it is recommended to download the dataset manually from the website. In that case place the downloaded zip under [data/era5_climate_data/](data/era5_climate_data/) and continue with step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da303d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-land-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_reanalysis\"],\n",
    "    \"variable\": [\n",
    "        \"2m_dewpoint_temperature\",\n",
    "        \"2m_temperature\",\n",
    "        \"soil_temperature_level_1\",\n",
    "        \"soil_temperature_level_4\",\n",
    "        \"snow_cover\",\n",
    "        \"snow_density\",\n",
    "        \"volumetric_soil_water_layer_1\",\n",
    "        \"volumetric_soil_water_layer_4\",\n",
    "        \"total_precipitation\",\n",
    "        \"soil_type\"\n",
    "    ],\n",
    "    \"year\": [\n",
    "        \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \n",
    "        \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \n",
    "        \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \n",
    "        \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \n",
    "        \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \n",
    "        \"2017\", \"2018\", \"2019\", \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"zip\"\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "result = client.retrieve(dataset, request).download(target_zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe6271",
   "metadata": {},
   "source": [
    "2. Unzip the dataset and extract the relevant file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ac7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(target_zip_file, \"r\") as zip_ref:\n",
    "    file_list = zip_ref.namelist()\n",
    "    data_file = next(f for f in file_list if \"data\" in f)\n",
    "    zip_ref.extract(data_file, \"data/era5_climate_data/\")\n",
    "    os.rename(f\"data/era5_climate_data/{data_file}\", f\"data/era5_climate_data/{dataset_name}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99744ee",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Load the dataset as an xarray.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb29927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_global = xr.open_dataset(f\"data/era5_climate_data/{dataset_name}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d0237",
   "metadata": {},
   "source": [
    "### Slicing the Dataset\n",
    "To make the analysis feasible I will focus on a \"slice\" of the earth that includes Europe and parts of North Africa. \n",
    "\n",
    "First we have to convert the longitude dimension of the dataset to run from -180 to 180 instead of 0 to 360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90de93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lon180(ds, lon_name='longitude'):\n",
    "    lon = ds[lon_name]\n",
    "    lon180 = ((lon + 180) % 360) - 180\n",
    "    return ds.assign_coords({lon_name: lon180}).sortby(lon_name)\n",
    "\n",
    "ds_era5_global = to_lon180(ds_era5_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e830efa",
   "metadata": {},
   "source": [
    "Now we slice from 11°W to 40°E and from 30°N to 72°N. Which converts to longitude=[-11, 40] and latitude=[72, 30]\n",
    "\n",
    "*Note: This step reduces the dataset from roughly 81GB to 3GB!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362bcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_sliced = ds_era5_global.sel(longitude=slice(-11, 40), latitude=slice(72, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6282c52",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "**Variables:**\n",
    "\n",
    "1. \"t2m\" --> 2 metre temperature in Kelvin\n",
    "2. \"d2m\" --> 2 metre dewpoint temperature in Kelvin\n",
    "3. \"stl1\" --> Soil temperature level 1 in Kelvin\n",
    "4. \"stl4\" --> Soil temperature level 4 in Kelvin\n",
    "5. \"snowc\" --> Snow cover in %\n",
    "6. \"rsn\" --> Snow density in kg/m³\n",
    "7. \"swvl1\" --> Volumetric soil water layer 1 in m³/m³\n",
    "8. \"swvl4\" --> Volumetric soil water layer 4 in m³/m³\n",
    "9. \"tp\" --> Total precipation in m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fd5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:     (valid_time: 348, latitude: 420, longitude: 510)\n",
      "Coordinates:\n",
      "    number      int64 8B ...\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 1992-01-01 ... 2020-12-01\n",
      "  * latitude    (latitude) float64 3kB 71.9 71.8 71.7 71.6 ... 30.2 30.1 30.0\n",
      "    expver      (valid_time) <U4 6kB ...\n",
      "  * longitude   (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.7 39.8 39.9\n",
      "Data variables:\n",
      "    d2m         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    stl1        (valid_time, latitude, longitude) float32 298MB ...\n",
      "    stl4        (valid_time, latitude, longitude) float32 298MB ...\n",
      "    snowc       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    rsn         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    swvl1       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    swvl4       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 298MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-09-24T09:07 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "print(ds_era5_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb21840",
   "metadata": {},
   "source": [
    "### Resampling to yearly data\n",
    "Since the PFT dataset only has yearly data, the climate data has to be resampled from monthly means to a yearly timescale. To not loose too much information by only having the yearly mean temperature, seasonal means are computed as well. \n",
    "\n",
    "The variable dewpoint temperature is not that useful on its own, so the Vapor Pressure Deficit (VPD) is calculated from it and included in the dataset.\n",
    "\n",
    "When aggregating the monthly data to yearly data the average has to be weighted by the amount of days in the month to get an exact yearly mean.\n",
    "\n",
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deceaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_weights_days_in_month(ds):\n",
    "    \"\"\"\n",
    "    Returns a DataArray of weights -> number of days in each month.\n",
    "    \"\"\"\n",
    "    # dt.days_in_month exists for both numpy and cftime time indexes in recent xarray/cftime\n",
    "    w = xr.DataArray(ds['time'].dt.days_in_month, coords={'time': ds['time']}, dims='time')\n",
    "    w.name = 'days_in_month'\n",
    "    return w\n",
    "\n",
    "def resample_time_weighted_mean(ds, freq, weights):\n",
    "    \"\"\"\n",
    "    Days-weighted mean over resample bins.\n",
    "\n",
    "    ds: Dataset or DataArray with time dimension\n",
    "    weights: 1D DataArray over 'time'\n",
    "    \"\"\"\n",
    "    def _group_mean(x):\n",
    "        w = weights.sel(time=x.time)\n",
    "        return x.weighted(w).mean('time')\n",
    "\n",
    "    return ds.resample(time=freq).map(_group_mean)\n",
    "\n",
    "def calculate_vpd(temp_kelvin, temp_dewpoint_kelvin):\n",
    "    \"\"\"\n",
    "    Vapor pressure deficit (VPD) from air temperature (K) and dewpoint (K).\n",
    "    Uses Tetens formula.\n",
    "\n",
    "    es(T)  = 6.112 * exp(17.67 * Tc / (Tc + 243.5))     [hPa]\n",
    "    ea(Td) = 6.112 * exp(17.67 * Tdc / (Tdc + 243.5))   [hPa]\n",
    "    VPD    = (es - ea) / 10                             [hPa]\n",
    "    \"\"\"\n",
    "    # Convert K to °C\n",
    "    temp_degrees = temp_kelvin - 273.15\n",
    "    temp_dewpoint_degrees = temp_dewpoint_kelvin - 273.15\n",
    "\n",
    "    # Tetens formula\n",
    "    es_hPa = 6.1078 * np.exp(17.269 * temp_degrees / (temp_degrees + 237.3))\n",
    "    ea_hPa = 6.1078 * np.exp(17.269 * temp_dewpoint_degrees / (temp_dewpoint_degrees + 237.3))\n",
    "\n",
    "    # Vapor Pressure Deficit es - ea in hPa\n",
    "    vpd_kPa = (es_hPa - ea_hPa)\n",
    "    return vpd_kPa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ff0ad",
   "metadata": {},
   "source": [
    "Use the helper functions to first calculate the VPD and add it to the existing dataset. Afterwards define which variables should be averaged when aggregating to yearly data and which variables to sum. Total precipation (tp) should be summed instead of averaged to represent the total precipation in a year instead of the average monthly precipation in a year.\n",
    "\n",
    "In the end the datasets are merged and the time dimension is now represented by an int for the year to match the PFT dataset.\n",
    "\n",
    "This takes a considerable amount of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_sliced = ds_era5_sliced.rename({'valid_time': 'time'})\n",
    "\n",
    "ds_era5_sliced['vpd'] = calculate_vpd(temp_kelvin=ds_era5_sliced['t2m'], temp_dewpoint_kelvin=ds_era5_sliced['d2m'])\n",
    "ds_era5_sliced['vpd'].attrs.update(units='hPa', long_name='Vapor pressure deficit')\n",
    "\n",
    "ds_mean = ds_era5_sliced[['t2m','stl1','stl4','snowc','rsn','swvl1','swvl4','vpd']] \n",
    "ds_sum = ds_era5_sliced[['tp']]\n",
    "\n",
    "time_weights = time_weights_days_in_month(ds_era5_sliced)\n",
    "\n",
    "yearly_means = resample_time_weighted_mean(ds_mean, 'YS', time_weights)\n",
    "yearly_sums = ds_sum.resample(time='YS').sum(skipna=True, min_count=1)\n",
    "\n",
    "ds_era5_sliced_yearly = xr.merge([yearly_means, yearly_sums])\n",
    "ds_era5_sliced_yearly = ds_era5_sliced_yearly\\\n",
    "                            .assign_coords(year=ds_era5_sliced_yearly['time'].dt.year)\\\n",
    "                            .swap_dims({'time':'year'}).drop_vars('time')\n",
    "\n",
    "ds_era5_sliced_yearly.to_netcdf(\"data/era5_climate_data/era5-land-yearly-means-sliced.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b600e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_sliced = ds_era5_sliced.rename({'valid_time': 'time'})\n",
    "\n",
    "ds_era5_sliced['vpd'] = calculate_vpd(temp_kelvin=ds_era5_sliced['t2m'], temp_dewpoint_kelvin=ds_era5_sliced['d2m'])\n",
    "ds_era5_sliced['vpd'].attrs.update(units='hPa', long_name='Vapor pressure deficit')\n",
    "\n",
    "ds_mean = ds_era5_sliced[['stl1','stl4','snowc','rsn','swvl1','swvl4','vpd']] \n",
    "ds_sum = ds_era5_sliced[['tp']]\n",
    "\n",
    "time_weights = time_weights_days_in_month(ds_era5_sliced)\n",
    "\n",
    "seas_means = resample_time_weighted_mean(ds_mean, 'QS-DEC', time_weights)\n",
    "seas_sums  = ds_sum.resample(time='QS-DEC').sum(skipna=True, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "991329ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:    (time: 117, latitude: 420, longitude: 510)\n",
      "Coordinates:\n",
      "    number     int64 8B 0\n",
      "  * latitude   (latitude) float64 3kB 71.9 71.8 71.7 71.6 ... 30.2 30.1 30.0\n",
      "  * longitude  (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.7 39.8 39.9\n",
      "  * time       (time) datetime64[ns] 936B 1991-12-01 1992-03-01 ... 2020-12-01\n",
      "Data variables:\n",
      "    stl1       (time, latitude, longitude) float64 200MB nan nan ... 284.9 284.9\n",
      "    stl4       (time, latitude, longitude) float64 200MB nan nan ... 298.5 298.7\n",
      "    snowc      (time, latitude, longitude) float64 200MB nan nan nan ... 0.0 0.0\n",
      "    rsn        (time, latitude, longitude) float64 200MB nan nan ... 100.0 100.0\n",
      "    swvl1      (time, latitude, longitude) float64 200MB nan nan ... 0.02919\n",
      "    swvl4      (time, latitude, longitude) float64 200MB nan nan ... 0.00267\n",
      "    vpd        (time, latitude, longitude) float64 200MB nan nan ... 5.589 5.597\n",
      "    tp         (time, latitude, longitude) float32 100MB nan nan ... 0.0003053\n",
      "    t2m        (time, latitude, longitude) float64 200MB ...\n"
     ]
    }
   ],
   "source": [
    "print(ds_era5_sliced_seas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9ef0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:           (latitude: 420, longitude: 510, year: 28)\n",
      "Coordinates:\n",
      "  * latitude          (latitude) float64 3kB 71.9 71.8 71.7 ... 30.2 30.1 30.0\n",
      "  * longitude         (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.8 39.9\n",
      "  * year              (year) int64 224B 1993 1994 1995 1996 ... 2018 2019 2020\n",
      "Data variables: (12/36)\n",
      "    stl1_season_DJF   (year, latitude, longitude) float64 48MB nan nan ... 283.6\n",
      "    stl1_season_MAM   (year, latitude, longitude) float64 48MB nan nan ... 297.1\n",
      "    stl1_season_JJA   (year, latitude, longitude) float64 48MB nan nan ... 308.3\n",
      "    stl1_season_SON   (year, latitude, longitude) float64 48MB nan nan ... 298.8\n",
      "    stl4_season_DJF   (year, latitude, longitude) float64 48MB nan nan ... 297.6\n",
      "    stl4_season_MAM   (year, latitude, longitude) float64 48MB nan nan ... 295.5\n",
      "    ...                ...\n",
      "    tp_season_JJA     (year, latitude, longitude) float32 24MB nan ... 2.275e-06\n",
      "    tp_season_SON     (year, latitude, longitude) float32 24MB nan ... 0.0001865\n",
      "    t2m_season_DJF    (year, latitude, longitude) float64 48MB nan nan ... 283.2\n",
      "    t2m_season_MAM    (year, latitude, longitude) float64 48MB nan nan ... 294.6\n",
      "    t2m_season_JJA    (year, latitude, longitude) float64 48MB nan nan ... 305.1\n",
      "    t2m_season_SON    (year, latitude, longitude) float64 48MB nan nan ... 297.4\n"
     ]
    }
   ],
   "source": [
    "def add_season_and_year_coords(ds):\n",
    "    \"\"\"\n",
    "    Tag each seasonal timestamp with:\n",
    "      - season: DJF/MAM/JJA/SON\n",
    "      - season_year: DJF assigned to the year of Jan/Feb (Dec gets +1)\n",
    "    \"\"\"\n",
    "    month = ds['time'].dt.month\n",
    "    season = xr.full_like(month, '', dtype=object)\n",
    "    season = xr.where(month==12, 'DJF', season)\n",
    "    season = xr.where(month== 3, 'MAM', season)\n",
    "    season = xr.where(month== 6, 'JJA', season)\n",
    "    season = xr.where(month== 9, 'SON', season)\n",
    "    season_year = ds['time'].dt.year.where(month != 12, ds['time'].dt.year + 1)\n",
    "    return ds.assign_coords(season=('time', season.data),\n",
    "                            season_year=('time', season_year.data))\n",
    "\n",
    "def seasons_to_wide_by_year(ds, limit_years=None, drop_aux=('number',)):\n",
    "    \"\"\"\n",
    "    Convert a seasonal 'long' dataset (1 timestamp per season) into a 'wide' dataset with\n",
    "    variables named <var>_season_<DJF/MAM/JJA/SON>, indexed by 'year'.\n",
    "    \"\"\"\n",
    "    # optional: drop stray coords like 'number' if present\n",
    "    for c in drop_aux:\n",
    "        if c in ds.coords:\n",
    "            ds = ds.drop_vars(c)\n",
    "\n",
    "    ds = add_season_and_year_coords(ds)\n",
    "\n",
    "    # pick the year index to use\n",
    "    years = np.unique(ds['season_year'].values)\n",
    "    if limit_years is not None:\n",
    "        y0, y1 = limit_years\n",
    "        years = years[(years >= y0) & (years <= y1)]\n",
    "\n",
    "    seasons = ['DJF','MAM','JJA','SON']\n",
    "    out = {}\n",
    "\n",
    "    for v in ds.data_vars:\n",
    "        for s in seasons:\n",
    "            sel = ds[v].where(ds['season'] == s, drop=True)\n",
    "\n",
    "            # move season-year into the index we want\n",
    "            sel = sel.assign_coords(\n",
    "                year=('time', ds['season_year'].where(ds['season']==s, drop=True).data)\n",
    "            ).swap_dims({'time':'year'}).drop_vars('time')\n",
    "\n",
    "            # drop the *conflicting* coords so variables can co-exist in one Dataset\n",
    "            sel = sel.reset_coords(['season','season_year'], drop=True)\n",
    "\n",
    "            # align to common year index\n",
    "            sel = sel.reindex(year=years)\n",
    "\n",
    "            out[f'{v}_season_{s}'] = sel\n",
    "\n",
    "    wide = xr.Dataset(out)\n",
    "\n",
    "    # annotate provenance\n",
    "    for name in wide.data_vars:\n",
    "        wide[name].attrs['cell_method'] = 'seasonal aggregate (DJF/MAM/JJA/SON); DJF labeled by Jan/Feb year'\n",
    "\n",
    "    return wide\n",
    "\n",
    "# --- Example usage ----------------------------------------------------------\n",
    "# ds_seasonal is your dataset with time at 1991-12-01, 1992-03-01, ..., 2020-12-01\n",
    "# If you only want 1992–2020 inclusive (since DJF 1992 needs Dec 1991), pass:\n",
    "wide_seasons = seasons_to_wide_by_year(ds_era5_sliced_seas, limit_years=(1993, 2020))\n",
    "\n",
    "print(wide_seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:           (year: 28, latitude: 420, longitude: 510)\n",
      "Coordinates:\n",
      "    number            int64 8B ...\n",
      "  * latitude          (latitude) float64 3kB 71.9 71.8 71.7 ... 30.2 30.1 30.0\n",
      "  * longitude         (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.8 39.9\n",
      "  * year              (year) int64 224B 1993 1994 1995 1996 ... 2018 2019 2020\n",
      "Data variables: (12/45)\n",
      "    vpd               (year, latitude, longitude) float64 48MB ...\n",
      "    tp                (year, latitude, longitude) float32 24MB ...\n",
      "    t2m               (year, latitude, longitude) float64 48MB ...\n",
      "    stl1              (year, latitude, longitude) float64 48MB ...\n",
      "    stl4              (year, latitude, longitude) float64 48MB ...\n",
      "    snowc             (year, latitude, longitude) float64 48MB ...\n",
      "    ...                ...\n",
      "    tp_season_JJA     (year, latitude, longitude) float32 24MB nan ... 2.275e-06\n",
      "    tp_season_SON     (year, latitude, longitude) float32 24MB nan ... 0.0001865\n",
      "    t2m_season_DJF    (year, latitude, longitude) float64 48MB nan nan ... 283.2\n",
      "    t2m_season_MAM    (year, latitude, longitude) float64 48MB nan nan ... 294.6\n",
      "    t2m_season_JJA    (year, latitude, longitude) float64 48MB nan nan ... 305.1\n",
      "    t2m_season_SON    (year, latitude, longitude) float64 48MB nan nan ... 297.4\n"
     ]
    }
   ],
   "source": [
    "yearly = xr.open_dataset(\"data/era5_climate_data/era5-land-yearly-means-sliced.nc\")\n",
    "yearly2 = yearly.sel(year=slice(1993, 2020))\n",
    "test = xr.merge([yearly2, wide_seasons])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bc723ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_netcdf(\"data/era5_climate_data/era5-land-yearly-means-sliced.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4848ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_sliced_seas.to_netcdf(\"data/era5_climate_data/seasonal-means.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d5d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def time_weights_days_in_month(ds):\n",
    "#     \"\"\"\n",
    "#     Returns a DataArray of weights -> number of days in each month.\n",
    "#     \"\"\"\n",
    "#     # dt.days_in_month exists for both numpy and cftime time indexes in recent xarray/cftime\n",
    "#     w = xr.DataArray(ds['time'].dt.days_in_month, coords={'time': ds['time']}, dims='time')\n",
    "#     w.name = 'days_in_month'\n",
    "#     return w\n",
    "\n",
    "# def resample_time_weighted_mean(ds, freq, weights):\n",
    "#     \"\"\"\n",
    "#     Compute time-weighted means over resample bins\n",
    "#     ds: Dataset or DataArray with time dimension\n",
    "#     weights: 1D DataArray over 'time'\n",
    "#     \"\"\"\n",
    "#     weights_nan_adjusted = xr.where(ds.notnull(), weights, 0).broadcast_like(ds)\n",
    "#     num = (ds * weights_nan_adjusted).resample(time=freq).sum(skipna=True, min_count=1)\n",
    "#     den = weights.resample(time=freq).sum(skipna=True)\n",
    "#     return num / den\n",
    "\n",
    "# def resample_time_weighted_mean(ds, freq, weights):\n",
    "#     \"\"\"\n",
    "#     Days-weighted mean over resample bins.\n",
    "\n",
    "#     ds: Dataset or DataArray with time dimension\n",
    "#     weights: 1D DataArray over 'time'\n",
    "#     \"\"\"\n",
    "#     def _group_mean(x):\n",
    "#         w = weights.sel(time=x.time)\n",
    "#         return x.weighted(w).mean('time')\n",
    "\n",
    "#     return ds.resample(time=freq).map(_group_mean)\n",
    "\n",
    "# def calculate_vpd(temp_kelvin, temp_dewpoint_kelvin):\n",
    "#     \"\"\"\n",
    "#     Vapor pressure deficit (VPD) from air temperature (K) and dewpoint (K).\n",
    "#     Uses Tetens formula.\n",
    "\n",
    "#     es(T)  = 6.112 * exp(17.67 * Tc / (Tc + 243.5))     [hPa]\n",
    "#     ea(Td) = 6.112 * exp(17.67 * Tdc / (Tdc + 243.5))   [hPa]\n",
    "#     VPD    = (es - ea) / 10                             [hPa]\n",
    "#     \"\"\"\n",
    "#     # Convert K to °C\n",
    "#     temp_degrees = temp_kelvin - 273.15\n",
    "#     temp_dewpoint_degrees = temp_dewpoint_kelvin - 273.15\n",
    "\n",
    "#     # Tetens formula\n",
    "#     es_hPa = 6.1078 * np.exp(17.269 * temp_degrees / (temp_degrees + 237.3))\n",
    "#     ea_hPa = 6.1078 * np.exp(17.269 * temp_dewpoint_degrees / (temp_dewpoint_degrees + 237.3))\n",
    "\n",
    "#     # Vapor Pressure Deficit es - ea in hPa\n",
    "#     vpd_kPa = (es_hPa - ea_hPa)\n",
    "#     return vpd_kPa\n",
    "\n",
    "\n",
    "# def add_season_and_year_coords(ds):\n",
    "#     \"\"\"\n",
    "#     After seasonal resampling with 'QS-DEC' (season starting on Dec 1),\n",
    "#     add two convenience coords:\n",
    "#     - 'season': DJF/MAM/JJA/SON string for each seasonal timestamp\n",
    "#     - 'season_year': integer year attributed to the season (DJF -> Jan/Feb year)\n",
    "#     \"\"\"\n",
    "#     month = ds['time'].dt.month\n",
    "#     season = xr.full_like(month, '', dtype=object)\n",
    "#     season = xr.where(month==12, 'DJF', season)\n",
    "#     season = xr.where(month== 3, 'MAM', season)\n",
    "#     season = xr.where(month== 6, 'JJA', season)\n",
    "#     season = xr.where(month== 9, 'SON', season)\n",
    "#     # The 'time' coordinate marks season start: 12->DJF, 3->MAM, 6->JJA, 9->SON\n",
    "#     # Assign DJF to the year of Jan/Feb (i.e., if month==12, season_year = year+1)\n",
    "#     season_year = ds['time'].dt.year.where(month != 12, ds['time'].dt.year + 1)\n",
    "\n",
    "#     ds = ds.assign_coords(season=('time', season.data))\n",
    "#     ds = ds.assign_coords(season_year=('time', season_year.data))\n",
    "#     return ds\n",
    "\n",
    "# def aggregate_era5_ds_OLD(ds, compute_vpd=True):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#       clim_yearly  : annual aggregates (means for state vars, sums for tp)\n",
    "#       clim_seasonal: seasonal aggregates (DJF/MAM/JJA/SON using QS-DEC)\n",
    "#     \"\"\"\n",
    "#     # lat, lon = infer_lat_lon_names(ds)\n",
    "#     time_weights = time_weights_days_in_month(ds)  # weights by # of days each month\n",
    "\n",
    "#     # Optional: compute VPD from t2m & d2m (strong ecological signal)\n",
    "#     if compute_vpd and 't2m' in ds and 'd2m' in ds:\n",
    "#         ds = ds.copy()\n",
    "#         ds['vpd'] = vpd_from_t_and_td(ds['t2m'], ds['d2m'])\n",
    "#         ds['vpd'].attrs.update(units='kPa', long_name='Vapor pressure deficit')\n",
    "\n",
    "#     # Variables to average (time-weighted means) vs to sum\n",
    "#     mean_vars = [v for v in ['t2m','stl1','stl4','snowc','rsn','swvl1','swvl4','vpd'] if v in ds]\n",
    "#     sum_vars  = [v for v in ['tp'] if v in ds]\n",
    "\n",
    "#     ds_mean = ds[mean_vars] if mean_vars else xr.Dataset()\n",
    "#     ds_sum  = ds[sum_vars]  if sum_vars  else xr.Dataset()\n",
    "\n",
    "#     # Annual:\n",
    "#     yearly_means = resample_time_weighted_mean(ds_mean, 'YS', time_weights) if mean_vars else xr.Dataset()\n",
    "#     yearly_sums  = ds_sum.resample(time='YS').sum(skipna=True)   if sum_vars  else xr.Dataset()\n",
    "#     ds_yearly  = xr.merge([yearly_means, yearly_sums])\n",
    "\n",
    "#     # Tidy attrs\n",
    "#     for v in ds_yearly.data_vars:\n",
    "#         if v != 'tp':\n",
    "#             ds_yearly[v].attrs['cell_method'] = 'time: mean within year (days-weighted)'\n",
    "#         else:\n",
    "#             ds_yearly[v].attrs['cell_method'] = 'time: sum within year'\n",
    " \n",
    "#     return ds_yearly\n",
    "\n",
    "# def aggregate_era5_ds(ds:xr.Dataset, compute_vpd=True, drop_d2m=False):\n",
    "#     \"\"\"\n",
    "#     Build ONE dataset with both annual and seasonal aggregates, named:\n",
    "#       <var>_yearly, <var>_season_DJF/MAM/JJA/SON\n",
    "#     Means are days-weighted; precipitation (tp) is summed.\n",
    "#     Output is indexed by 'year' (integer) plus your spatial dims.\n",
    "#     \"\"\"\n",
    "#     ds = ds.rename({'valid_time': 'time'})\n",
    "\n",
    "#     # Optional VPD\n",
    "#     if compute_vpd and 't2m' in ds and 'd2m' in ds:\n",
    "#         ds = ds.copy()\n",
    "#         ds['vpd'] = vpd_from_t_and_td(ds['t2m'], ds['d2m'])\n",
    "#         ds['vpd'].attrs.update(units='kPa', long_name='Vapor pressure deficit')\n",
    "\n",
    "#     # What to average vs sum\n",
    "#     mean_vars = [v for v in ['t2m','stl1','stl4','snowc','rsn','swvl1','swvl4','vpd'] if v in ds]\n",
    "#     sum_vars  = [v for v in ['tp'] if v in ds]\n",
    "\n",
    "#     ds_mean = ds[mean_vars] if mean_vars else xr.Dataset()\n",
    "#     ds_sum  = ds[sum_vars]  if sum_vars  else xr.Dataset()\n",
    "\n",
    "#     # Time weights\n",
    "#     w = time_weights_days_in_month(ds)\n",
    "\n",
    "#     # --- ANNUAL ---\n",
    "#     yearly_means = resample_time_weighted_mean(ds_mean, 'YS', w) if mean_vars else xr.Dataset()\n",
    "#     yearly_sums  = ds_sum.resample(time='YS').sum(skipna=True)   if sum_vars  else xr.Dataset()\n",
    "#     ds_yearly    = xr.merge([yearly_means, yearly_sums])\n",
    "#     ds_yearly    = ds_yearly.assign_coords(year=ds_yearly['time'].dt.year)\\\n",
    "#                              .swap_dims({'time':'year'}).drop_vars('time')\n",
    "\n",
    "#     # --- SEASONAL (DJF/MAM/JJA/SON, DJF labeled with Jan/Feb year) ---\n",
    "#     seas_means = resample_time_weighted_mean(ds_mean, 'QS-DEC', w) if mean_vars else xr.Dataset()\n",
    "#     seas_sums  = ds_sum.resample(time='QS-DEC').sum(skipna=True)    if sum_vars  else xr.Dataset()\n",
    "#     ds_season  = xr.merge([seas_means, seas_sums])\n",
    "#     ds_season  = add_season_and_year_coords(ds_season)\n",
    "\n",
    "#     # Build a single \"wide\" dataset: copy yearly vars + expand seasons as separate vars\n",
    "#     out = {}\n",
    "\n",
    "#     # Annual variables\n",
    "#     for v in ds_yearly.data_vars:\n",
    "#         out[f'{v}_yearly'] = ds_yearly[v]\n",
    "\n",
    "#     # Seasonal variables\n",
    "#     seasons = ['DJF','MAM','JJA','SON']\n",
    "#     # use annual year index for alignment\n",
    "#     years = ds_yearly['year']\n",
    "\n",
    "#     for v in ds_season.data_vars:\n",
    "#         for s in seasons:\n",
    "#             sel = ds_season[v].where(ds_season['season'] == s, drop=True)\n",
    "#             # carry season_year as the axis, then align to full year index\n",
    "#             sel = sel.assign_coords(year=('time', ds_season['season_year']\n",
    "#                                           .where(ds_season['season'] == s, drop=True).data))\n",
    "#             sel = sel.swap_dims({'time':'year'}).drop_vars('time')\n",
    "#             sel = sel.reindex(year=years)\n",
    "#             out[f'{v}_season_{s}'] = sel\n",
    "\n",
    "#     combined = xr.Dataset(out)\n",
    "\n",
    "#     # Optional: drop dewpoint from the final dataset but keep derived VPD\n",
    "#     if drop_d2m and 'd2m' in combined:\n",
    "#         combined = combined.drop_vars('d2m')\n",
    "\n",
    "#     # Attributes for transparency\n",
    "#     for name in combined.data_vars:\n",
    "#         if name.startswith('tp_'):\n",
    "#             combined[name].attrs['cell_method'] = 'time: sum within year/season'\n",
    "#         elif name.endswith('_yearly'):\n",
    "#             combined[name].attrs['cell_method'] = 'time: mean within year (days-weighted)'\n",
    "#         else:\n",
    "#             combined[name].attrs['cell_method'] = 'time: mean within season (days-weighted)'\n",
    "\n",
    "#     return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74d5743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the values equal? True\n",
      "Are the values close (allclose)? True\n"
     ]
    }
   ],
   "source": [
    "test = xr.open_dataset(\"data/era5_climate_data/annual_means_test.nc\")\n",
    "\n",
    "# Test if test[\"t2m\"] and yearly_means[\"t2m\"] have the same values\n",
    "print(\"Are the values equal?\", xr.DataArray.equals(test[\"t2m\"], yearly_means))\n",
    "print(\"Are the values close (allclose)?\", np.allclose(test[\"t2m\"], yearly_means, equal_nan=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa24665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'swvl1' (time: 29, latitude: 420, longitude: 510)> Size: 50MB\n",
      "[6211800 values with dtype=float64]\n",
      "Coordinates:\n",
      "    number     int64 8B ...\n",
      "  * latitude   (latitude) float64 3kB 71.9 71.8 71.7 71.6 ... 30.2 30.1 30.0\n",
      "  * longitude  (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.7 39.8 39.9\n",
      "  * time       (time) datetime64[ns] 232B 1992-01-01 1993-01-01 ... 2020-01-01\n"
     ]
    }
   ],
   "source": [
    "print(test[\"swvl1\"])\n",
    "\n",
    "x = test[\"swvl1\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatepft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
