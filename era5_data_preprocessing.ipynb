{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9dad51",
   "metadata": {},
   "source": [
    "# ERA5 Data Preprocessing\n",
    "This script is doing the necessary preprocessing of the ERA5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1539dab",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5671d",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is publicly available on [Copernicus.eu](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land-monthly-means?tab=download), but needs a free account to download. There are two possible ways of accessing it: API or manual download. When the internet connection is unstable the api dowload option tends to fail. In that case, manually download the dataset with the configuration as described in the api-call code below and put the downloaded zip under [data/era5_climate_data/](data/era5_climate_data/) and skip **Download Dataset with API**.\n",
    "\n",
    "First we define the filename and location where to put or load the data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b150e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"era5-land-monthly-means-1992-2020\"\n",
    "target_zip_file = f'data/era5_climate_data/{dataset_name}.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07215273",
   "metadata": {},
   "source": [
    "#### Download Dataset with API\n",
    "**Requirement:** Setup the cdsapi with your account as described here: https://cds.climate.copernicus.eu/how-to-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da303d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"reanalysis-era5-land-monthly-means\"\n",
    "request = {\n",
    "    \"product_type\": [\"monthly_averaged_reanalysis\"],\n",
    "    \"variable\": [\n",
    "        \"2m_dewpoint_temperature\",\n",
    "        \"2m_temperature\",\n",
    "        \"soil_temperature_level_1\",\n",
    "        \"soil_temperature_level_4\",\n",
    "        \"snow_cover\",\n",
    "        \"snow_density\",\n",
    "        \"volumetric_soil_water_layer_1\",\n",
    "        \"volumetric_soil_water_layer_4\",\n",
    "        \"total_precipitation\",\n",
    "        \"soil_type\"\n",
    "    ],\n",
    "    \"year\": [\n",
    "        \"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \n",
    "        \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \n",
    "        \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \n",
    "        \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \n",
    "        \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \n",
    "        \"2017\", \"2018\", \"2019\", \"2020\"\n",
    "    ],\n",
    "    \"month\": [\n",
    "        \"01\", \"02\", \"03\", \"04\", \"05\", \"06\",\n",
    "        \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"\n",
    "    ],\n",
    "    \"time\": [\"00:00\"],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"zip\"\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "result = client.retrieve(dataset, request).download(target_zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99744ee",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "1. Unzip the dataset and extract the relevant file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab80192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(target_zip_file, \"r\") as zip_ref:\n",
    "    file_list = zip_ref.namelist()\n",
    "    data_file = next(f for f in file_list if \"data\" in f)\n",
    "    zip_ref.extract(data_file, \"data/era5_climate_data/\")\n",
    "    os.rename(f\"data/era5_climate_data/{data_file}\", f\"data/era5_climate_data/{dataset_name}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7683938",
   "metadata": {},
   "source": [
    "2. Open the era5 data as an *xarray.Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb29927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_global = xr.open_dataset(f\"data/era5_climate_data/{dataset_name}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d0237",
   "metadata": {},
   "source": [
    "### Slicing the Dataset\n",
    "To make the analysis feasible I will focus on a \"slice\" of the earth that includes Europe and parts of North Africa. \n",
    "\n",
    "First we have to convert the longitude dimension of the dataset to run from -180 to 180 instead of 0 to 360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90de93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lon180(ds, lon_name='longitude'):\n",
    "    lon = ds[lon_name]\n",
    "    lon180 = ((lon + 180) % 360) - 180\n",
    "    return ds.assign_coords({lon_name: lon180}).sortby(lon_name)\n",
    "\n",
    "ds_era5_global = to_lon180(ds_era5_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e830efa",
   "metadata": {},
   "source": [
    "Now we slice from 11°W to 40°E and from 30°N to 72°N. Which converts to longitude=[-11, 40] and latitude=[72, 30]\n",
    "\n",
    "*Note: This step reduces the dataset from roughly 81GB to 3GB!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362bcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_sliced = ds_era5_global.sel(longitude=slice(-11, 40), latitude=slice(72, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6282c52",
   "metadata": {},
   "source": [
    "### Dataset Information\n",
    "**Variables:**\n",
    "\n",
    "1. \"t2m\" --> 2 metre temperature in Kelvin\n",
    "2. \"d2m\" --> 2 metre dewpoint temperature in Kelvin\n",
    "3. \"stl1\" --> Soil temperature level 1 in Kelvin\n",
    "4. \"stl4\" --> Soil temperature level 4 in Kelvin\n",
    "5. \"snowc\" --> Snow cover in %\n",
    "6. \"rsn\" --> Snow density in kg/m³\n",
    "7. \"swvl1\" --> Volumetric soil water layer 1 in m³/m³\n",
    "8. \"swvl4\" --> Volumetric soil water layer 4 in m³/m³\n",
    "9. \"tp\" --> Total precipation in m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fd5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 3GB\n",
      "Dimensions:     (valid_time: 348, latitude: 420, longitude: 510)\n",
      "Coordinates:\n",
      "    number      int64 8B ...\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 1992-01-01 ... 2020-12-01\n",
      "  * latitude    (latitude) float64 3kB 71.9 71.8 71.7 71.6 ... 30.2 30.1 30.0\n",
      "    expver      (valid_time) <U4 6kB ...\n",
      "  * longitude   (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.7 39.8 39.9\n",
      "Data variables:\n",
      "    d2m         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    stl1        (valid_time, latitude, longitude) float32 298MB ...\n",
      "    stl4        (valid_time, latitude, longitude) float32 298MB ...\n",
      "    snowc       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    rsn         (valid_time, latitude, longitude) float32 298MB ...\n",
      "    swvl1       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    swvl4       (valid_time, latitude, longitude) float32 298MB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 298MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-09-24T09:07 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "print(ds_era5_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb21840",
   "metadata": {},
   "source": [
    "### Resampling to yearly data\n",
    "Since the PFT dataset only has yearly data, the climate data has to be resampled from monthly means to a yearly timescale. To not loose too much information by only having the yearly mean temperature, seasonal means are computed as well. \n",
    "\n",
    "The variable dewpoint temperature is not that useful on its own, so the Vapor Pressure Deficit (VPD) is calculated from it and included in the dataset.\n",
    "\n",
    "When aggregating the monthly data to yearly data the average has to be weighted by the amount of days in the month to get an exact yearly mean.\n",
    "\n",
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deceaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_weights_days_in_month(ds):\n",
    "    \"\"\"\n",
    "    Returns a DataArray of weights -> number of days in each month.\n",
    "    \"\"\"\n",
    "    w = xr.DataArray(ds['time'].dt.days_in_month, coords={'time': ds['time']}, dims='time')\n",
    "    w.name = 'days_in_month'\n",
    "    return w\n",
    "\n",
    "def resample_time_weighted_mean(ds, freq, weights):\n",
    "    \"\"\"\n",
    "    Days-weighted mean over resample bins.\n",
    "\n",
    "    ds: Dataset or DataArray with time dimension\n",
    "    weights: 1D DataArray over 'time'\n",
    "    \"\"\"\n",
    "    def _group_mean(x):\n",
    "        w = weights.sel(time=x.time)\n",
    "        return x.weighted(w).mean('time')\n",
    "\n",
    "    return ds.resample(time=freq).map(_group_mean)\n",
    "\n",
    "def calculate_vpd(temp_kelvin, temp_dewpoint_kelvin):\n",
    "    \"\"\"\n",
    "    Vapor pressure deficit (VPD) from air temperature (K) and dewpoint (K).\n",
    "    Uses Tetens formula.\n",
    "\n",
    "    es(T)  = 6.112 * exp(17.67 * Tc / (Tc + 243.5))     [hPa]\n",
    "    ea(Td) = 6.112 * exp(17.67 * Tdc / (Tdc + 243.5))   [hPa]\n",
    "    VPD    = (es - ea)                                  [hPa]\n",
    "    \"\"\"\n",
    "    # Convert K to °C\n",
    "    temp_degrees = temp_kelvin - 273.15\n",
    "    temp_dewpoint_degrees = temp_dewpoint_kelvin - 273.15\n",
    "\n",
    "    # Tetens formula\n",
    "    es_hPa = 6.1078 * np.exp(17.269 * temp_degrees / (temp_degrees + 237.3))\n",
    "    ea_hPa = 6.1078 * np.exp(17.269 * temp_dewpoint_degrees / (temp_dewpoint_degrees + 237.3))\n",
    "\n",
    "    # Vapor Pressure Deficit es - ea in hPa\n",
    "    vpd_hPa = (es_hPa - ea_hPa)\n",
    "    return vpd_hPa\n",
    "\n",
    "def add_season_and_year_coords(ds):\n",
    "    \"\"\"\n",
    "    Tag each seasonal timestamp with:\n",
    "      - season: DJF/MAM/JJA/SON\n",
    "      - season_year: DJF assigned to the year of Jan/Feb (Dec gets +1)\n",
    "    \"\"\"\n",
    "    month = ds['time'].dt.month\n",
    "    season = xr.full_like(month, '', dtype=object)\n",
    "    season = xr.where(month==12, 'DJF', season)\n",
    "    season = xr.where(month== 3, 'MAM', season)\n",
    "    season = xr.where(month== 6, 'JJA', season)\n",
    "    season = xr.where(month== 9, 'SON', season)\n",
    "    season_year = ds['time'].dt.year.where(month != 12, ds['time'].dt.year + 1)\n",
    "    return ds.assign_coords(season=('time', season.data),\n",
    "                            season_year=('time', season_year.data))\n",
    "\n",
    "def convert_to_yearly_time(ds, limit_years=None):\n",
    "    \"\"\"\n",
    "    Convert a seasonal 'long' dataset (1 timestamp per season) into a 'wide' dataset with\n",
    "    variables named <var>_season_<DJF/MAM/JJA/SON>, indexed by 'year'.\n",
    "    \"\"\"\n",
    "    ds = add_season_and_year_coords(ds)\n",
    "\n",
    "    # Pick the year index to use\n",
    "    years = np.unique(ds['season_year'].values)\n",
    "    if limit_years is not None:\n",
    "        y0, y1 = limit_years\n",
    "        years = years[(years >= y0) & (years <= y1)]\n",
    "\n",
    "    # Define the seasonal prefixes\n",
    "    seasons = ['DJF','MAM','JJA','SON']\n",
    "    out = {}\n",
    "\n",
    "    for v in ds.data_vars:\n",
    "        for s in seasons:\n",
    "            sel = ds[v].where(ds['season'] == s, drop=True)\n",
    "\n",
    "            # Move season-year into the yearly index \n",
    "            sel = sel.assign_coords(\n",
    "                year=('time', ds['season_year'].where(ds['season']==s, drop=True).data)\n",
    "            ).swap_dims({'time':'year'}).drop_vars('time')\n",
    "\n",
    "            # Drop the conflicting coords\n",
    "            sel = sel.reset_coords(['season','season_year'], drop=True)\n",
    "\n",
    "            # Align to common year index\n",
    "            sel = sel.reindex(year=years)\n",
    "\n",
    "            out[f'{v}_season_{s}'] = sel\n",
    "\n",
    "    yearly = xr.Dataset(out)\n",
    "\n",
    "    # Annotate with meta information on naming conventions\n",
    "    for name in yearly.data_vars:\n",
    "        yearly[name].attrs['cell_method'] = 'seasonal aggregate (DJF/MAM/JJA/SON); DJF labeled by Jan/Feb year'\n",
    "\n",
    "    return yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ff0ad",
   "metadata": {},
   "source": [
    "##### Calculation\n",
    "Use the helper functions to first calculate the VPD and add it to the existing dataset. Afterwards define which variables should be averaged when aggregating to yearly data and which variables to sum. Total precipation (tp) should be summed instead of averaged to represent the total precipation in a year instead of the average monthly precipation in a year.\n",
    "\n",
    "In the end the datasets are merged and the time dimension is now represented by an int for the year to match the PFT dataset.\n",
    "\n",
    "*Info: This takes a considerable amount of time to run.*\n",
    "\n",
    "**1. Normal yearly aggregation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename time variable\n",
    "ds_era5_sliced = ds_era5_sliced.rename({'valid_time': 'time'})\n",
    "\n",
    "# Calculate Vapor Pressure Deficit\n",
    "ds_era5_sliced['vpd'] = calculate_vpd(temp_kelvin=ds_era5_sliced['t2m'], temp_dewpoint_kelvin=ds_era5_sliced['d2m'])\n",
    "ds_era5_sliced['vpd'].attrs.update(units='hPa', long_name='Vapor pressure deficit')\n",
    "\n",
    "# define which variables to take mean from and which to sum\n",
    "ds_mean = ds_era5_sliced[['t2m','stl1','stl4','snowc','rsn','swvl1','swvl4','vpd']] \n",
    "ds_sum = ds_era5_sliced[['tp']]\n",
    "\n",
    "# Calculate the time weights\n",
    "time_weights = time_weights_days_in_month(ds_era5_sliced)\n",
    "\n",
    "# Calculate yearly aggregations\n",
    "yearly_means = resample_time_weighted_mean(ds_mean, 'YS', time_weights)\n",
    "yearly_sums = ds_sum.resample(time='YS').sum(skipna=True, min_count=1)\n",
    "\n",
    "\n",
    "\n",
    "# Merge the aggregated results\n",
    "ds_era5_sliced_yearly = xr.merge([yearly_means, yearly_sums])\n",
    "ds_era5_sliced_yearly = ds_era5_sliced_yearly\\\n",
    "                            .assign_coords(year=ds_era5_sliced_yearly['time'].dt.year)\\\n",
    "                            .swap_dims({'time':'year'}).drop_vars('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42512f7",
   "metadata": {},
   "source": [
    "**2. Seasonal Aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seasonal aggregations\n",
    "seasonal_means = resample_time_weighted_mean(ds_mean, 'QS-DEC', time_weights)\n",
    "seasonal_sums  = ds_sum.resample(time='QS-DEC').sum(skipna=True, min_count=1)\n",
    "\n",
    "# Merge the aggregated results\n",
    "ds_era5_sliced_seasonal = xr.merge([seasonal_means, seasonal_sums])\n",
    "\n",
    "# Restructure time coord from seasonal to yearly with variables with different naming\n",
    "# Have to reduce the years by 1 because of overlap between years with the season December-January-February into the previous year\n",
    "ds_era5_sliced_seasonal = convert_to_yearly_time(ds_era5_sliced_seasonal, limit_years=(1993, 2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa920a3",
   "metadata": {},
   "source": [
    "**3. Combine yearly and seasonal variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991329ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:    (time: 117, latitude: 420, longitude: 510)\n",
      "Coordinates:\n",
      "    number     int64 8B 0\n",
      "  * latitude   (latitude) float64 3kB 71.9 71.8 71.7 71.6 ... 30.2 30.1 30.0\n",
      "  * longitude  (longitude) float64 4kB -11.0 -10.9 -10.8 ... 39.7 39.8 39.9\n",
      "  * time       (time) datetime64[ns] 936B 1991-12-01 1992-03-01 ... 2020-12-01\n",
      "Data variables:\n",
      "    stl1       (time, latitude, longitude) float64 200MB nan nan ... 284.9 284.9\n",
      "    stl4       (time, latitude, longitude) float64 200MB nan nan ... 298.5 298.7\n",
      "    snowc      (time, latitude, longitude) float64 200MB nan nan nan ... 0.0 0.0\n",
      "    rsn        (time, latitude, longitude) float64 200MB nan nan ... 100.0 100.0\n",
      "    swvl1      (time, latitude, longitude) float64 200MB nan nan ... 0.02919\n",
      "    swvl4      (time, latitude, longitude) float64 200MB nan nan ... 0.00267\n",
      "    vpd        (time, latitude, longitude) float64 200MB nan nan ... 5.589 5.597\n",
      "    tp         (time, latitude, longitude) float32 100MB nan nan ... 0.0003053\n",
      "    t2m        (time, latitude, longitude) float64 200MB ...\n"
     ]
    }
   ],
   "source": [
    "# Align years by slicing this data as well\n",
    "ds_era5_sliced_yearly = ds_era5_sliced_yearly.sel(year=slice(1993, 2020))\n",
    "\n",
    "# Merge Datsets\n",
    "ds_era5_yearly_merged = xr.merge([ds_era5_sliced_yearly, ds_era5_sliced_seasonal])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329d776",
   "metadata": {},
   "source": [
    "**4. Save as file for later use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5_yearly_merged.to_netcdf(\"data/era5_climate_data/era5-land-yearly-means-sliced.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatepft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
